{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": [
     "worksheet-0"
    ]
   },
   "source": [
    "# Project 2: Supervised Learning\n",
    "### Building a Student Intervention System"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": [
     "worksheet-0"
    ]
   },
   "source": [
    "*Note to self:* this is the source of the data: https://archive.ics.uci.edu/ml/datasets/Student+Performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": [
     "worksheet-0"
    ]
   },
   "source": [
    "## 1. Classification vs Regression\n",
    "\n",
    "Your goal is to identify students who might need early intervention - which type of supervised machine learning problem is this, classification or regression? Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": [
     "worksheet-0"
    ]
   },
   "source": [
    "Identifying students who might need early intervention is a classification problem as you are sorting students into classes (*needs intervention*, *doesn't need intervention*) rather than trying to predict a quantitative value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": [
     "worksheet-0"
    ]
   },
   "source": [
    "## 2. Exploring the Data\n",
    "\n",
    "Let's go ahead and read in the student dataset first.\n",
    "\n",
    "_To execute a code cell, click inside it and press **Shift+Enter**._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "autoscroll": "json-false",
    "collapsed": false,
    "ein.tags": [
     "worksheet-0"
    ]
   },
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import numpy\n",
    "import pandas as pd\n",
    "\n",
    "# my imports\n",
    "import matplotlib.pyplot as plot\n",
    "import seaborn\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.metrics import f1_score, make_scorer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "autoscroll": "json-false",
    "collapsed": false,
    "ein.tags": [
     "worksheet-0"
    ]
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "autoscroll": "json-false",
    "collapsed": false,
    "ein.tags": [
     "worksheet-0"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Student data read successfully!\n"
     ]
    }
   ],
   "source": [
    "# Read student data\n",
    "student_data = pd.read_csv(\"student-data.csv\")\n",
    "print(\"Student data read successfully!\")\n",
    "# Note: The last column 'passed' is the target/label, all other are feature columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": [
     "worksheet-0"
    ]
   },
   "source": [
    "Now, can you find out the following facts about the dataset?\n",
    "- Total number of students\n",
    "- Number of students who passed\n",
    "- Number of students who failed\n",
    "- Graduation rate of the class (%)\n",
    "- Number of features\n",
    "\n",
    "_Use the code block below to compute these values. Instructions/steps are marked using **TODO**s._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "autoscroll": "json-false",
    "collapsed": false,
    "ein.tags": [
     "worksheet-0"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of students: 395\n",
      "Number of students who passed: 265\n",
      "Number of students who failed: 130\n",
      "Number of features: 31\n",
      "Graduation rate of the class: 0.67%\n"
     ]
    }
   ],
   "source": [
    "n_students = student_data.shape[0]\n",
    "n_features = student_data.shape[1]\n",
    "n_passed = sum(student_data.passed.map({'no': 0, 'yes': 1}))\n",
    "n_failed = n_students - n_passed\n",
    "grad_rate = n_passed/float(n_students)\n",
    "print \"Total number of students: {}\".format(n_students)\n",
    "print \"Number of students who passed: {}\".format(n_passed)\n",
    "print \"Number of students who failed: {}\".format(n_failed)\n",
    "print \"Number of features: {}\".format(n_features)\n",
    "print \"Graduation rate of the class: {:.2f}%\".format(grad_rate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "autoscroll": "json-false",
    "collapsed": false,
    "ein.tags": [
     "worksheet-0"
    ]
   },
   "outputs": [],
   "source": [
    "types = student_data.dtypes\n",
    "categoricals = [column for column in types.index if types.loc[column] == object]\n",
    "numericals = [column for column in types.index if column not in categoricals]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "autoscroll": "json-false",
    "collapsed": false,
    "ein.tags": [
     "worksheet-0"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Categorical Variables: 18\n",
      "Numeric Variables: 13\n"
     ]
    }
   ],
   "source": [
    "print(\"Categorical Variables: {0}\".format(len(categoricals)))\n",
    "print(\"Numeric Variables: {0}\".format(len(numericals)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "autoscroll": "json-false",
    "collapsed": false,
    "ein.tags": [
     "worksheet-0"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "school\tGP,MS\n",
      "sex\tF,M\n",
      "address\tU,R\n",
      "famsize\tGT3,LE3\n",
      "Pstatus\tA,T\n",
      "Mjob\tat_home,health,other,services,teacher\n",
      "Fjob\tteacher,other,services,health,at_home\n",
      "reason\tcourse,other,home,reputation\n",
      "guardian\tmother,father,other\n",
      "schoolsup\tyes,no\n",
      "famsup\tno,yes\n",
      "paid\tno,yes\n",
      "activities\tno,yes\n",
      "nursery\tyes,no\n",
      "higher\tyes,no\n",
      "internet\tno,yes\n",
      "romantic\tno,yes\n",
      "passed\tno,yes\n"
     ]
    }
   ],
   "source": [
    "for categorical in categoricals:\n",
    "    print('{0}\\t{1}'.format(categorical, ','.join(student_data[categorical].unique())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "autoscroll": "json-false",
    "collapsed": false,
    "ein.tags": [
     "worksheet-0"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f1f462aea50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f1f463e80d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f1f46539b90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f1f468f7a90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f1f46796bd0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f1f46a30050>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f1f46a41190>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f1f46b5d350>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f1f46c63450>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f1f46d9ced0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f1f46f23d90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f1f470cc350>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f1f471e0290>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f1f472daa50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f1f4777c150>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f1f47556390>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f1f476feb10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f1f47a50810>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "categorical_data = student_data[categoricals]\n",
    "for categorical in categoricals:\n",
    "    grid = seaborn.FacetGrid(categorical_data, col='passed')\n",
    "    grid = grid.map(seaborn.countplot, categorical)\n",
    "    grid.fig.suptitle('passed vs {0}'.format(categorical))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": [
     "worksheet-0"
    ]
   },
   "source": [
    "Surprisingly, Females were less likely to pass than males. `family size` seems to influence passing, as does parental cohabitation, whether parents worked jobs other than services, health, teacher, or at home, reason for taking the course, whether they were paid, whether they had internet access at home."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "autoscroll": "json-false",
    "collapsed": false,
    "ein.tags": [
     "worksheet-0"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f1f47a50350>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/charon/.virtualenvs/machinelearning/local/lib/python2.7/site-packages/seaborn/categorical.py:2125: UserWarning: The boxplot API has been changed. Attempting to adjust your arguments for the new API (which might not work). Please update your code. See the version 0.6 release notes for more info.\n",
      "  warnings.warn(msg, UserWarning)\n"
     ]
    }
   ],
   "source": [
    "figure = plot.figure(figsize=(10,8))\n",
    "axe = figure.gca()\n",
    "axe.set_title('numeric variables')\n",
    "lines = seaborn.boxplot(x=student_data[numericals], ax=axe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "autoscroll": "json-false",
    "collapsed": false,
    "ein.tags": [
     "worksheet-0"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f1f45aff3d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "numerical_data = student_data[numericals]\n",
    "figure = plot .figure(figsize=(10,8))\n",
    "axe = figure.gca()\n",
    "axe = numerical_data.plot(kind='kde', ax=axe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "autoscroll": "json-false",
    "collapsed": false,
    "ein.tags": [
     "worksheet-0"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f1f45bfd6d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/charon/.virtualenvs/machinelearning/lib/python2.7/site-packages/ipykernel/__main__.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  from ipykernel import kernelapp as app\n"
     ]
    }
   ],
   "source": [
    "from pandas.tools.plotting import parallel_coordinates\n",
    "numerical_data['passed'] = student_data['passed']\n",
    "figure = plot.figure(figsize=(10,10))\n",
    "axe = figure.gca()\n",
    "subplot = parallel_coordinates(numerical_data, 'passed', ax=axe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "autoscroll": "json-false",
    "collapsed": false,
    "ein.tags": [
     "worksheet-0"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yes    0.670886\n",
      "no     0.329114\n",
      "Name: passed, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "passed = student_data.passed.value_counts()/student_data.shape[0]\n",
    "print(passed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "autoscroll": "json-false",
    "collapsed": false,
    "ein.tags": [
     "worksheet-0"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f1f47a9afd0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/charon/.virtualenvs/machinelearning/local/lib/python2.7/site-packages/matplotlib/collections.py:590: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  if self._edgecolors == str('face'):\n"
     ]
    }
   ],
   "source": [
    "grid = seaborn.FacetGrid(student_data, col='passed', size=8)\n",
    "grid = grid.map_dataframe(lambda data, color: seaborn.heatmap(data.corr(), linewidths=0))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": [
     "worksheet-0"
    ]
   },
   "source": [
    "The highest corellations appear to be Dalc (workday alcohol consumption) and Walc (weekend alcohol consumption), along with Medu (mother's education) and Fedu (father's education)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "autoscroll": "json-false",
    "collapsed": false,
    "ein.tags": [
     "worksheet-0"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f1f477bf350>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "figure = plot.figure(figsize=(10,8))\n",
    "axe = figure.gca()\n",
    "axe.set_ylabel('proportion')\n",
    "axe.set_title(\"Count of Passing Students\")\n",
    "grid = seaborn.countplot(student_data.passed, ax=axe)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": [
     "worksheet-0"
    ]
   },
   "source": [
    "## 3. Preparing the Data\n",
    "In this section, we will prepare the data for modeling, training and testing.\n",
    "\n",
    "### Identify feature and target columns\n",
    "It is often the case that the data you obtain contains non-numeric features. This can be a problem, as most machine learning algorithms expect numeric data to perform computations with.\n",
    "\n",
    "Let's first separate our data into feature and target columns, and see if any features are non-numeric.<br/>\n",
    "**Note**: For this dataset, the last column (`'passed'`) is the target or label we are trying to predict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "autoscroll": "json-false",
    "collapsed": false,
    "ein.tags": [
     "worksheet-0"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature column(s):-\n",
      "['school', 'sex', 'age', 'address', 'famsize', 'Pstatus', 'Medu', 'Fedu', 'Mjob', 'Fjob', 'reason', 'guardian', 'traveltime', 'studytime', 'failures', 'schoolsup', 'famsup', 'paid', 'activities', 'nursery', 'higher', 'internet', 'romantic', 'famrel', 'freetime', 'goout', 'Dalc', 'Walc', 'health', 'absences']\n",
      "Target column: passed\n",
      "\n",
      "Feature values:-\n",
      "  school sex  age address famsize Pstatus  Medu  Fedu     Mjob      Fjob  \\\n",
      "0     GP   F   18       U     GT3       A     4     4  at_home   teacher   \n",
      "1     GP   F   17       U     GT3       T     1     1  at_home     other   \n",
      "2     GP   F   15       U     LE3       T     1     1  at_home     other   \n",
      "3     GP   F   15       U     GT3       T     4     2   health  services   \n",
      "4     GP   F   16       U     GT3       T     3     3    other     other   \n",
      "\n",
      "    ...    higher internet  romantic  famrel  freetime goout Dalc Walc health  \\\n",
      "0   ...       yes       no        no       4         3     4    1    1      3   \n",
      "1   ...       yes      yes        no       5         3     3    1    1      3   \n",
      "2   ...       yes      yes        no       4         3     2    2    3      3   \n",
      "3   ...       yes      yes       yes       3         2     2    1    1      5   \n",
      "4   ...       yes       no        no       4         3     2    1    2      5   \n",
      "\n",
      "  absences  \n",
      "0        6  \n",
      "1        4  \n",
      "2       10  \n",
      "3        2  \n",
      "4        4  \n",
      "\n",
      "[5 rows x 30 columns]\n"
     ]
    }
   ],
   "source": [
    "# Extract feature (X) and target (y) columns\n",
    "feature_cols = list(student_data.columns[:-1])  # all columns but last are features\n",
    "target_col = student_data.columns[-1]  # last column is the target/label\n",
    "print \"Feature column(s):-\\n{}\".format(feature_cols)\n",
    "print \"Target column: {}\".format(target_col)\n",
    "\n",
    "X_all = student_data[feature_cols]  # feature values for all students\n",
    "y_all = student_data[target_col]  # corresponding targets/labels\n",
    "print \"\\nFeature values:-\"\n",
    "print X_all.head()  # print the first 5 rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": [
     "worksheet-0"
    ]
   },
   "source": [
    "### Preprocess feature columns\n",
    "\n",
    "As you can see, there are several non-numeric columns that need to be converted! Many of them are simply `yes`/`no`, e.g. `internet`. These can be reasonably converted into `1`/`0` (binary) values.\n",
    "\n",
    "Other columns, like `Mjob` and `Fjob`, have more than two values, and are known as _categorical variables_. The recommended way to handle such a column is to create as many columns as possible values (e.g. `Fjob_teacher`, `Fjob_other`, `Fjob_services`, etc.), and assign a `1` to one of them and `0` to all others.\n",
    "\n",
    "These generated columns are sometimes called _dummy variables_, and we will use the [`pandas.get_dummies()`](http://pandas.pydata.org/pandas-docs/stable/generated/pandas.get_dummies.html?highlight=get_dummies#pandas.get_dummies) function to perform this transformation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "autoscroll": "json-false",
    "collapsed": false,
    "ein.tags": [
     "worksheet-0"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed feature columns (48):-\n",
      "['school_GP', 'school_MS', 'sex_F', 'sex_M', 'age', 'address_R', 'address_U', 'famsize_GT3', 'famsize_LE3', 'Pstatus_A', 'Pstatus_T', 'Medu', 'Fedu', 'Mjob_at_home', 'Mjob_health', 'Mjob_other', 'Mjob_services', 'Mjob_teacher', 'Fjob_at_home', 'Fjob_health', 'Fjob_other', 'Fjob_services', 'Fjob_teacher', 'reason_course', 'reason_home', 'reason_other', 'reason_reputation', 'guardian_father', 'guardian_mother', 'guardian_other', 'traveltime', 'studytime', 'failures', 'schoolsup', 'famsup', 'paid', 'activities', 'nursery', 'higher', 'internet', 'romantic', 'famrel', 'freetime', 'goout', 'Dalc', 'Walc', 'health', 'absences']\n"
     ]
    }
   ],
   "source": [
    "# Preprocess feature columns\n",
    "def preprocess_features(X):\n",
    "    outX = pd.DataFrame(index=X.index)  # output dataframe, initially empty\n",
    "\n",
    "    # Check each column\n",
    "    for col, col_data in X.iteritems():\n",
    "        # If data type is non-numeric, try to replace all yes/no values with 1/0\n",
    "        if col_data.dtype == object:\n",
    "            col_data = col_data.replace(['yes', 'no'], [1, 0])\n",
    "        # Note: This should change the data type for yes/no columns to int\n",
    "\n",
    "        # If still non-numeric, convert to one or more dummy variables\n",
    "        if col_data.dtype == object:\n",
    "            col_data = pd.get_dummies(col_data, prefix=col)  # e.g. 'school' => 'school_GP', 'school_MS'\n",
    "\n",
    "        outX = outX.join(col_data)  # collect column(s) in output dataframe\n",
    "\n",
    "    return outX\n",
    "\n",
    "X_all = preprocess_features(X_all)\n",
    "print \"Processed feature columns ({}):-\\n{}\".format(len(X_all.columns), list(X_all.columns))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": [
     "worksheet-0"
    ]
   },
   "source": [
    "### Split data into training and test sets\n",
    "\n",
    "So far, we have converted all _categorical_ features into numeric values. In this next step, we split the data (both features and corresponding labels) into training and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "autoscroll": "json-false",
    "collapsed": false,
    "ein.tags": [
     "worksheet-0"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set: 300 samples\n",
      "Test set: 95 samples\n"
     ]
    }
   ],
   "source": [
    "# First, decide how many training vs test samples you want\n",
    "num_all = student_data.shape[0]  # same as len(student_data)\n",
    "num_train = 300  # about 75% of the data\n",
    "num_test = num_all - num_train\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_all, y_all,\n",
    "                                                    test_size=num_test,\n",
    "                                                    train_size=num_train)\n",
    "\n",
    "\n",
    "print \"Training set: {} samples\".format(X_train.shape[0])\n",
    "print \"Test set: {} samples\".format(X_test.shape[0])\n",
    "# Note: If you need a validation set, extract it from within training data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": [
     "worksheet-0"
    ]
   },
   "source": [
    "## 4. Training and Evaluating Models\n",
    "Choose 3 supervised learning models that are available in scikit-learn, and appropriate for this problem. For each model:\n",
    "\n",
    "- What are the general applications of this model? What are its strengths and weaknesses?\n",
    "- Given what you know about the data so far, why did you choose this model to apply?\n",
    "- Fit this model to the training data, try to predict labels (for both training and test sets), and measure the F<sub>1</sub> score. Repeat this process with different training set sizes (100, 200, 300), keeping test set constant.\n",
    "\n",
    "Produce a table showing training time, prediction time, F<sub>1</sub> score on training set and F<sub>1</sub> score on test set, for each training set size.\n",
    "\n",
    "Note: You need to produce 3 such tables - one for each model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": [
     "worksheet-0"
    ]
   },
   "source": [
    "### LogisticRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": [
     "worksheet-0"
    ]
   },
   "source": [
    "The first supervised learning model that I've chosen is `Logistic Regression <http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html#sklearn.linear_model.LogisticRegression>`_. Logistic Regression uses numeric data to predict binary categorical values, matching our inputs (after transformation) and outputs here. It is a linear classification model and so does best when the data is linearly separable, although it can be made to work as long as the features are pairwise-separable (Alpaydin, 2010). Logistic Regression has the advantage of being computationally cheap, reasonable to implement, and is interpretable but has the disadvantage that it is prone to underfitting (Harrington, 2012).\n",
    "\n",
    "Logistic Regression uses the log-likelihood of the model to decide how good it is and tries to improve it by choosing weights that maximize the log-likelihood (Witten & Eibe, 2005). Logistic Regression calculates the probability that a target-feature is 1 using the `logistic (sigmoid) function` (Alpaydin, 2010)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "autoscroll": "json-false",
    "collapsed": false,
    "ein.tags": [
     "worksheet-0"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.Latex object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%latex\n",
    "P(y=1|x) = sigmoid(W^Tx + w_0)\n",
    "= \\frac{1}{1 + e^{-(W^Tx + w_0)}}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": [
     "worksheet-0"
    ]
   },
   "source": [
    "The sklearn implementation also supports regularization and thus can be used for feature selection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": [
     "worksheet-0"
    ]
   },
   "source": [
    "### Random Forests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": [
     "worksheet-0"
    ]
   },
   "source": [
    "The second learning model that I will use will be `Random Forests<http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html#sklearn.ensemble.RandomForestClassifier>`_. This is an ensemble learner that combines predictions from multiple decision trees, each trained on a separate data set.\n",
    "\n",
    "Decision Trees have several advantages, including the fact that they are easily interpretable, can sometimes fit complex data more easily than linear models, and don't require dummy variable. They are, however, generally not as accurate (James G. et al., 2013).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": [
     "worksheet-0"
    ]
   },
   "source": [
    "The idea behind using ensemble learners is that any particular model has a bias built into it based on its assumptions - when the assumptions are wrong it will perform poorly. You can improve performance by combining base-learners each of which has a different bias so that (ideally) no instance of the data will cause a majority of the learners to perform poorly, even if each performs poorly in some instances. For combining of models to work, there has to be enough diversity that they don't all fail on the same data (Alpaydin 2010).\n",
    "\n",
    "The first way to introduce diversity is through *bagging (boostrap aggregation)* where each tree (base-learner) is given a data set that is constructed by re-sampling (with replacement) from the training-data.\n",
    "\n",
    "The next way that diversity is introduced is by using a random samples of features whenever a split is made, rather than choosing the best split from all the features (the number of features used is near the square-root of the number of total features). By keeping the number of features small it reduces the likelihood that more influential features will dominate the splitting early on, causing the trees to be too similar (Gareth G. et al., 2013). This use of sub-sets of features in splitting is what makes it a random-forest (rather than just bagged trees).\n",
    "\n",
    "Predictions are made by having each tree make a prediction and then the average of the predictions is used for the final prediction for the entire forest. Using these methods improves the performance over using an individual tree, but the ensemble is no longer interpretable the way a tree would be."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": [
     "worksheet-0"
    ]
   },
   "source": [
    "### K-Nearest Neighbors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": [
     "worksheet-0"
    ]
   },
   "source": [
    "My final predictor will use *K-Nearest Neighbors (KNN)* classification. It is a fairly straight-forward method that doesn't build a model in the sense that the other two methods do. Instead KNN stores the training data and when asked to make a prediction finds the *k* number of points in the training data that are 'closest' to the input and calculates the probability of a classification (say *y=1*) as the fraction of the k-neighbors that are of that class. Thus if k=4 and three of the chosen neighbors are classified as *1* then the predicted class will be *1*, because the majority of the neighbors were 1.\n",
    "\n",
    "Unlike Logistic Regression, KNN doesn't require linear separability and unlike some other methods also makes no assumption about the distribution of the data (it is *non-parametric*). This makes it better in some cases, but how accurate it is depends on the choice of *k*. If *k* is too small it will tend to overfit the training data and if *k* is too large, it will become too rigid. Besides the difficulty in choosing *k*, because it is non-parametric it's not possible to inspect the model to decide which features are important. Additionally, since it's non-parametric, KNN needs more data to be accurate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "autoscroll": "json-false",
    "collapsed": false,
    "ein.tags": [
     "worksheet-0"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training DecisionTreeClassifier...\n",
      "Done!\n",
      "Training time (secs): 0.004\n",
      "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
      "            max_features=None, max_leaf_nodes=None, min_samples_leaf=1,\n",
      "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "            presort=False, random_state=None, splitter='best')\n"
     ]
    }
   ],
   "source": [
    "# Train a model\n",
    "import time\n",
    "\n",
    "def train_classifier(clf, X_train, y_train):\n",
    "    print \"Training {}...\".format(clf.__class__.__name__)\n",
    "    start = time.time()\n",
    "    clf.fit(X_train, y_train)\n",
    "    end = time.time()\n",
    "    print \"Done!\\nTraining time (secs): {:.3f}\".format(end - start)\n",
    "\n",
    "# TODO: Choose a model, import it and instantiate an object\n",
    "from sklearn import tree\n",
    "clf = tree.DecisionTreeClassifier()\n",
    "\n",
    "# Fit model to training data\n",
    "train_classifier(clf, X_train, y_train)  # note: using entire training set here\n",
    "print clf  # you can inspect the learned model by printing it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "autoscroll": "json-false",
    "collapsed": false,
    "ein.tags": [
     "worksheet-0"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting labels using DecisionTreeClassifier...\n",
      "Done!\n",
      "Prediction time (secs): 0.001\n",
      "F1 score for training set: 1.0\n"
     ]
    }
   ],
   "source": [
    "# Predict on training set and compute F1 score\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "def predict_labels(clf, features, target):\n",
    "    print \"Predicting labels using {}...\".format(clf.__class__.__name__)\n",
    "    start = time.time()\n",
    "    y_pred = clf.predict(features)\n",
    "    end = time.time()\n",
    "    print \"Done!\\nPrediction time (secs): {:.3f}\".format(end - start)\n",
    "    return f1_score(target.values, y_pred, pos_label='yes')\n",
    "\n",
    "train_f1_score = predict_labels(clf, X_train, y_train)\n",
    "print \"F1 score for training set: {}\".format(train_f1_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "autoscroll": "json-false",
    "collapsed": false,
    "ein.tags": [
     "worksheet-0"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting labels using DecisionTreeClassifier...\n",
      "Done!\n",
      "Prediction time (secs): 0.001\n",
      "F1 score for test set: 0.588235294118\n"
     ]
    }
   ],
   "source": [
    "# Predict on test data\n",
    "print \"F1 score for test set: {}\".format(predict_labels(clf, X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "autoscroll": "json-false",
    "collapsed": false,
    "ein.tags": [
     "worksheet-0"
    ]
   },
   "outputs": [],
   "source": [
    "# Train and predict using different training set sizes\n",
    "def train_predict(clf, X_train, y_train, X_test, y_test):\n",
    "    print \"------------------------------------------\"\n",
    "    print \"Training set size: {}\".format(len(X_train))\n",
    "    train_classifier(clf, X_train, y_train)\n",
    "    print \"F1 score for training set: {}\".format(predict_labels(clf, X_train, y_train))\n",
    "    print \"F1 score for test set: {}\".format(predict_labels(clf, X_test, y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "autoscroll": "json-false",
    "collapsed": false,
    "ein.tags": [
     "worksheet-0"
    ]
   },
   "outputs": [],
   "source": [
    "class Classifier(object):\n",
    "    \"\"\"\n",
    "    Trains, predicts, evaluates classifier using f1 score\n",
    "    \"\"\"\n",
    "    def __init__(self, classifier, x_train, y_train, x_test, y_test, delim='\\t'):\n",
    "        \"\"\"\n",
    "        :param:\n",
    "         - `classifier`: sklearn classifier object\n",
    "         - `x_train`: feature training data\n",
    "         - `y_train`: target training data\n",
    "         - `x_test`: feature test data\n",
    "         - `y_test`: target test data\n",
    "         - `delim`: separator for the table row\n",
    "        \"\"\"\n",
    "        self.clf = classifier\n",
    "        self._classifier = None\n",
    "        self.x_train = x_train\n",
    "        self.x_test = x_test\n",
    "        self.y_train = y_train\n",
    "        self.y_test = y_test\n",
    "        self._f1_train = None\n",
    "        self._f1_test = None\n",
    "        self.delim = delim\n",
    "        self._table_row = None\n",
    "        self._training_time = None\n",
    "        self._prediction_time = None\n",
    "        return\n",
    "\n",
    "    @property\n",
    "    def f1_train(self):\n",
    "        \"\"\"\n",
    "        :return: F1 score using training data\n",
    "        \"\"\"\n",
    "        if self._f1_train is None:\n",
    "            predictions, time_ = self.predict(self.x_train)\n",
    "            self._f1_train = self.f1_score(predictions, self.y_train)\n",
    "        return self._f1_train\n",
    "\n",
    "    @property\n",
    "    def f1_test(self):\n",
    "        \"\"\"\n",
    "        :return: f1 score for test-set predictions\n",
    "        :postcondition: self.prection_time set\n",
    "        \"\"\"\n",
    "        if self._f1_test is None:\n",
    "            predictions, self._prediction_time = self.predict(self.x_test)\n",
    "            self._f1_test = self.f1_score(predictions, self.y_test)\n",
    "        return self._f1_test\n",
    "\n",
    "    @property\n",
    "    def prediction_time(self):\n",
    "        \"\"\"\n",
    "        :return: prediction time for test data\n",
    "        \"\"\"\n",
    "        if self._prediction_time is None:\n",
    "            predictions, self._prediction_time = self.predict(self.x_test)\n",
    "            self._f1_test = self.f1_score(predictions, self.y_test)\n",
    "        return self._prediction_time\n",
    "\n",
    "    @property\n",
    "    def training_time(self):\n",
    "        \"\"\"\n",
    "        :return: training time in seconds\n",
    "        \"\"\"\n",
    "        if self._training_time is None:\n",
    "            start = time.time()\n",
    "            self._classifier = self.clf.fit(self.x_train, self.y_train)\n",
    "            self._training_time = time.time() - start\n",
    "        return self._training_time\n",
    "        \n",
    "    @property\n",
    "    def classifier(self):\n",
    "        \"\"\"\n",
    "        :return: trained classifier\n",
    "        \"\"\"\n",
    "        if self._classifier is None:\n",
    "            start = time.time()\n",
    "            self._classifier = self.clf.fit(self.x_train, self.y_train)\n",
    "            self._training_time = time.time() - start\n",
    "        return self._classifier\n",
    "\n",
    "    def f1_score(self, predictions, target):\n",
    "        \"\"\"\n",
    "        :param:\n",
    "         - `predictions`: predicted values for model\n",
    "         - `target`: actual outcomes from data\n",
    "        :return: f1 score for predictions\n",
    "        \"\"\"\n",
    "        return f1_score(target.values, predictions, pos_label='yes')\n",
    "\n",
    "    def predict(self, features):\n",
    "        \"\"\"\n",
    "        :param:\n",
    "         - `features`: array of feature data\n",
    "        :return: predicted values, time to execute\n",
    "        \"\"\"\n",
    "        start = time.time()\n",
    "        predictions = self.classifier.predict(features)\n",
    "        elapsed = time.time() - start\n",
    "        return predictions, elapsed\n",
    "\n",
    "    def train_and_predict(self):\n",
    "        \"\"\"\n",
    "        :return: time, f1 score for training and testing data\n",
    "        \"\"\"\n",
    "        train_predictions, train_predictions_time = self.predict(self.x_train)\n",
    "        train_f1_score = self.f1_score(train_predictions, self.y_train)\n",
    "        \n",
    "        test_predictions, test_predictions_time = self.predict(self.x_test)\n",
    "        test_f1_score = self.f1_score(test_predictions, self.y_test)\n",
    "        return (train_predictions_time, train_f1_score,\n",
    "                test_predictions_time, test_f1_score)\n",
    "    \n",
    "    @property\n",
    "    def table_row(self):\n",
    "        \"\"\"\n",
    "        :return: string of training size, training time, prediction time, f1 train, f1 test\n",
    "        \"\"\"\n",
    "        if self._table_row is None:\n",
    "            self._table_row = self.delim.join([str(len(self.x_train))] +\n",
    "                                              [\"{0:.4f}\".format(item) for item in (self.training_time,\n",
    "                                                                                   self.prediction_time,\n",
    "                                                                                   self.f1_train,\n",
    "                                                                                   self.f1_test)])\n",
    "        return self._table_row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "autoscroll": "json-false",
    "collapsed": false,
    "ein.tags": [
     "worksheet-0"
    ]
   },
   "outputs": [],
   "source": [
    "def train_and_predict(clf):\n",
    "    scores = []\n",
    "    for size in range(100, 400, 100):\n",
    "        x_train_subset, y_train_subset = X_train[:size], y_train[:size]\n",
    "        classifier = Classifier(clf, x_train_subset, y_train_subset,\n",
    "                                X_test, y_test, delim='\\t\\t')\n",
    "        # train_time, train_score, test_time, test_score = classifier.train_and_predict()\n",
    "        # print('\\t\\t\\t'.join([str(size)] + ['{0:.2f}'.format(item) for item in (classifier.training_time,\n",
    "        #                                                                        train_score,\n",
    "        #                                                                        test_time,\n",
    "        #                                                                        test_score)]))\n",
    "        print(classifier.table_row)\n",
    "        scores.append((classifier.f1_test, size))\n",
    "    return max(scores)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "autoscroll": "json-false",
    "collapsed": false,
    "ein.tags": [
     "worksheet-0"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--------------------------------------------------------------------------------\n",
      "best score: 0.80, best_size: 200\n",
      "\n",
      "KNeighborsClassifier\n",
      "================================================================================\n",
      "Size\t\tTime(t)\t\tTime(p)\t\tTrain F1\tTest F1\n",
      "--------------------------------------------------------------------------------\n",
      "100\t\t0.0008\t\t0.0016\t\t0.8176\t\t0.8054\n",
      "200\t\t0.0009\t\t0.0022\t\t0.8664\t\t0.8082\n",
      "300\t\t0.0009\t\t0.0028\t\t0.8604\t\t0.8243\n",
      "--------------------------------------------------------------------------------\n",
      "best score: 0.82, best_size: 300\n",
      "================================================================================\n",
      "\n",
      "Ranked by Score\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Classifier                     score    training-size\n",
      "--------------------------  --------  ---------------\n",
      "KNeighborsClassifier        0.824324              300\n",
      "LogisticRegression          0.814286              200\n",
      "SVC                         0.805195              300\n",
      "SGDClassifier               0.805031              300\n",
      "GradientBoostingClassifier  0.8                   200\n",
      "RandomForestClassifier      0.787879              300\n",
      "AdaBoostClassifier          0.787879              100\n",
      "GaussianNB                  0.779412              200\n",
      "DecisionTreeClassifier      0.761905              100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "300\t\t0.1404\t\t0.0006\t\t0.9663\t\t0.7538"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "200\t\t0.1142\t\t0.0006\t\t0.9858\t\t0.8000"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--------------------------------------------------------------------------------\n",
      "best score: 0.79, best_size: 100\n",
      "\n",
      "GradientBoostingClassifier\n",
      "================================================================================\n",
      "Size\t\tTime(t)\t\tTime(p)\t\tTrain F1\tTest F1\n",
      "--------------------------------------------------------------------------------\n",
      "100\t\t0.0823\t\t0.0006\t\t1.0000\t\t0.7704"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "300\t\t0.1106\t\t0.0060\t\t0.8505\t\t0.7852"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "200\t\t0.1708\t\t0.0077\t\t0.9034\t\t0.7801"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "200\t\t0.0009\t\t0.0002\t\t0.4286\t\t0.4138\n",
      "300\t\t0.0010\t\t0.0002\t\t0.8024\t\t0.8050\n",
      "--------------------------------------------------------------------------------\n",
      "best score: 0.81, best_size: 300\n",
      "\n",
      "AdaBoostClassifier\n",
      "================================================================================\n",
      "Size\t\tTime(t)\t\tTime(p)\t\tTrain F1\tTest F1\n",
      "--------------------------------------------------------------------------------\n",
      "100\t\t0.0952\t\t0.0061\t\t0.9859\t\t0.7879"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--------------------------------------------------------------------------------\n",
      "best score: 0.79, best_size: 300\n",
      "\n",
      "SVC\n",
      "================================================================================\n",
      "Size\t\tTime(t)\t\tTime(p)\t\tTrain F1\tTest F1\n",
      "--------------------------------------------------------------------------------\n",
      "100\t\t0.0019\t\t0.0011\t\t0.8485\t\t0.7975\n",
      "200\t\t0.0046\t\t0.0017\t\t0.8598\t\t0.8052\n",
      "300\t\t0.0092\t\t0.0023\t\t0.8529\t\t0.8052\n",
      "--------------------------------------------------------------------------------\n",
      "best score: 0.81, best_size: 300\n",
      "\n",
      "GaussianNB\n",
      "================================================================================\n",
      "Size\t\tTime(t)\t\tTime(p)\t\tTrain F1\tTest F1\n",
      "--------------------------------------------------------------------------------\n",
      "100\t\t0.0008\t\t0.0004\t\t0.5053\t\t0.3373\n",
      "200\t\t0.0009\t\t0.0004\t\t0.8333\t\t0.7794\n",
      "300\t\t0.0009\t\t0.0003\t\t0.8009\t\t0.7786\n",
      "--------------------------------------------------------------------------------\n",
      "best score: 0.78, best_size: 200\n",
      "\n",
      "SGDClassifier\n",
      "================================================================================\n",
      "Size\t\tTime(t)\t\tTime(p)\t\tTrain F1\tTest F1\n",
      "--------------------------------------------------------------------------------\n",
      "100\t\t0.0006\t\t0.0002\t\t0.7287\t\t0.7414"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "200\t\t0.0250\t\t0.0014\t\t0.9929\t\t0.7681\n",
      "300\t\t0.0233\t\t0.0013\t\t0.9975\t\t0.7879"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "LogisticRegression\n",
      "================================================================================\n",
      "Size\t\tTime(t)\t\tTime(p)\t\tTrain F1\tTest F1\n",
      "--------------------------------------------------------------------------------\n",
      "100\t\t0.0023\t\t0.0003\t\t0.8725\t\t0.8000\n",
      "200\t\t0.0039\t\t0.0004\t\t0.8366\t\t0.8143\n",
      "300\t\t0.0046\t\t0.0003\t\t0.8273\t\t0.7971\n",
      "--------------------------------------------------------------------------------\n",
      "best score: 0.81, best_size: 200\n",
      "\n",
      "DecisionTreeClassifier\n",
      "================================================================================\n",
      "Size\t\tTime(t)\t\tTime(p)\t\tTrain F1\tTest F1\n",
      "--------------------------------------------------------------------------------\n",
      "100\t\t0.0013\t\t0.0003\t\t1.0000\t\t0.7619\n",
      "200\t\t0.0022\t\t0.0003\t\t1.0000\t\t0.6504\n",
      "300\t\t0.0029\t\t0.0003\t\t1.0000\t\t0.6218\n",
      "--------------------------------------------------------------------------------\n",
      "best score: 0.76, best_size: 100\n",
      "\n",
      "RandomForestClassifier\n",
      "================================================================================\n",
      "Size\t\tTime(t)\t\tTime(p)\t\tTrain F1\tTest F1\n",
      "--------------------------------------------------------------------------------\n",
      "100\t\t0.0227\t\t0.0013\t\t1.0000\t\t0.7737"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\n",
    "from sklearn import svm\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "classifiers = [LogisticRegression(), tree.DecisionTreeClassifier(), RandomForestClassifier(),\n",
    "               svm.SVC(), GaussianNB(), SGDClassifier(), AdaBoostClassifier(),\n",
    "               GradientBoostingClassifier(), KNeighborsClassifier()]\n",
    "best_scores = []\n",
    "line_width = 80\n",
    "for classifier in classifiers:\n",
    "    print('')\n",
    "    print(classifier.__class__.__name__)\n",
    "    print(\"=\" * line_width)\n",
    "    print(\"Size\\t\\tTime(t)\\t\\tTime(p)\\t\\tTrain F1\\tTest F1\")\n",
    "    print('-' * line_width)\n",
    "    best_score, best_size = train_and_predict(classifier)\n",
    "    print(\"-\" * line_width)\n",
    "    print(\"best score: {0:.2f}, best_size: {1}\".format(best_score, best_size))\n",
    "    best_scores.append((best_score,classifier.__class__.__name__, best_size))\n",
    "print(\"=\" * line_width)\n",
    "print('')\n",
    "print(\"Ranked by Score\")\n",
    "print('~' * line_width)\n",
    "from tabulate import tabulate\n",
    "table = [[score[1], score[0], score[-1]] for index,score in enumerate(sorted(best_scores, reverse=True))]\n",
    "print(tabulate(table, headers='Classifier score training-size'.split()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": [
     "worksheet-0"
    ]
   },
   "source": [
    "## 5. Choosing the Best Model\n",
    "\n",
    "- Based on the experiments you performed earlier, in 1-2 paragraphs explain to the board of supervisors what single model you chose as the best model. Which model is generally the most appropriate based on the available data, limited resources, cost, and performance?\n",
    "- In 1-2 paragraphs explain to the board of supervisors in layman's terms how the final model chosen is supposed to work (for example if you chose a Decision Tree or Support Vector Machine, how does it make a prediction).\n",
    "- Fine-tune the model. Use Gridsearch with at least one important parameter tuned and with at least 3 settings. Use the entire training set for this.\n",
    "- What is the model's final F<sub>1</sub> score?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "autoscroll": "json-false",
    "collapsed": false,
    "ein.tags": [
     "worksheet-0"
    ]
   },
   "outputs": [],
   "source": [
    "y_train_numeric = y_train.replace('yes no'.split(), [1, 0])\n",
    "y_test_numeric = y_test.replace('yes no'.split(), [1, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "autoscroll": "json-false",
    "collapsed": false,
    "ein.tags": [
     "worksheet-0"
    ]
   },
   "outputs": [],
   "source": [
    "class LRClassifier(object):\n",
    "    \"\"\"\n",
    "    holds the LogisticRegression classifier\n",
    "    \"\"\"\n",
    "    def __init__(self, c_range, score_function=f1_score, n_jobs=-1, folds=10,\n",
    "                 training_features=X_train, training_targets=y_train_numeric,\n",
    "                 test_features=X_test, test_targets=y_test_numeric):\n",
    "        \"\"\"\n",
    "        :param:\n",
    "         - `c_range`: range of 'C' values for grid search\n",
    "         - `score_function`: function to maximize\n",
    "         - `n_jobs`: number of parallel jobs for the grid search\n",
    "         - `folds`: number of cross validation folds to use\n",
    "         - `training_features`: array of training feature-data\n",
    "         - `training_targets`: array of training target-values\n",
    "         - `test_features`: array of testing feature-data\n",
    "         - `test_targets`: array of testing target-data\n",
    "        \"\"\"\n",
    "        self.c_range = c_range\n",
    "        self.n_jobs = n_jobs\n",
    "        self.folds = folds\n",
    "        self.score_function = score_function\n",
    "        self.training_features = training_features\n",
    "        self.training_targets = training_targets\n",
    "        self.test_features = test_features\n",
    "        self.test_targets = test_targets\n",
    "        \n",
    "        self._scorer = None\n",
    "        self._model = None\n",
    "        self._grid = None\n",
    "        self._parameters = None\n",
    "        return\n",
    "\n",
    "    @property\n",
    "    def parameters(self):\n",
    "        \"\"\"\n",
    "        :return: dict of grid search parameters\n",
    "        \"\"\"\n",
    "        if self._parameters is None:\n",
    "            self._parameters = {'penalty': ('l1', 'l2'),\n",
    "                                'C': self.c_range}\n",
    "        return self._parameters\n",
    "    \n",
    "    @property\n",
    "    def scorer(self):\n",
    "        \"\"\"\n",
    "        :return: scorer for the grid search\n",
    "        \"\"\"\n",
    "        if self._scorer is None:\n",
    "            self._scorer = make_scorer(self.score_function)\n",
    "        return self._scorer\n",
    "\n",
    "    @property\n",
    "    def model(self):\n",
    "        \"\"\"\n",
    "        :return: LogisticRegression object\n",
    "        \"\"\"\n",
    "        if self._model is None:\n",
    "            self._model = LogisticRegression()\n",
    "        return self._model\n",
    "\n",
    "    @property\n",
    "    def grid(self):\n",
    "        \"\"\"\n",
    "        :return: GridSearchCV object with best model\n",
    "        \"\"\"\n",
    "        if self._grid is None:\n",
    "            self._grid = GridSearchCV(self.model,\n",
    "                                      param_grid=self.parameters,\n",
    "                                      scoring=self.scorer,\n",
    "                                      cv=self.folds,\n",
    "                                      n_jobs=self.n_jobs)\n",
    "            self._grid.fit(self.training_features, self.training_targets)\n",
    "        return self._grid\n",
    "\n",
    "    def print_columns(self):\n",
    "        \"\"\"\n",
    "        prints non-zero coefficients in descending order\n",
    "        \"\"\"\n",
    "        coefficients = self.grid.best_estimator_.coef_[0]\n",
    "        sorted_coefficients = sorted((column for column in coefficients), reverse=True)\n",
    "        for coefficient in sorted_coefficients:\n",
    "            if abs(coefficient) > 0:\n",
    "                index = numpy.where(coefficients == coefficient)[0][0]\n",
    "                print(X_test.columns[index], coefficient)\n",
    "        return\n",
    "\n",
    "    def print_best(self):\n",
    "        print('Parameters')\n",
    "        print(self.grid.best_params_)\n",
    "        print('\\nF1 Score')\n",
    "        print(self.grid.score(self.test_features, self.test_targets))\n",
    "        print('\\ncoefficients')\n",
    "        self.print_columns()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "autoscroll": "json-false",
    "collapsed": false,
    "ein.tags": [
     "worksheet-0"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'penalty': 'l1', 'C': 0.080000000000000002}\n",
      "\n",
      "F1 Score\n",
      "0.797297297297\n",
      "\n",
      "coefficients\n",
      "('age', 0.04631275424685484)\n",
      "('Medu', 0.04122577352087383)\n",
      "('famrel', 0.02989095441447533)\n",
      "('absences', -0.020337195557860503)\n",
      "('failures', -0.6111689390192977)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters\n"
     ]
    }
   ],
   "source": [
    "grid_01 = LRClassifier(numpy.arange(.01, 1.1, .01))\n",
    "grid_01.print_best()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  },
  "name": "student_intervention.ipynb"
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
